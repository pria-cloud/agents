# ML1 â€” Learning Loop & Fineâ€‘Tune Pipeline Specification

**Document status:** Draft v0.1 Â· JuneÂ 2025\
**Owner:** CEOâ€¯(Perâ€¯Swedenborg)\
**Purpose:** Describe the endâ€‘toâ€‘end machineâ€‘learning lifecycle that powers PRIAâ€™s agent improvement loop: data extraction, feature engineering, nightly adapter fineâ€‘tuning (QLoRA), evaluation, promotion, rollback, and cost governance.  The pipeline must be fully automated, auditable, and aligned with the leanâ€‘team constraints (â‰¤â€¯15 engineers).

---

## Table of Contents

1. Objectives & Success Metrics
2. Data Sources & Privacy Classification
3. Feature Engineering & Dataset Build
4. Model Architecture & Adapter Strategy
5. Fineâ€‘Tune Orchestration Pipeline
6. Evaluation & Safety Filters
7. Model Registry & Promotion Policy
8. Cost Model & GPU Resource Planning
9. Prompt Blocks & Automation Scripts
10. Operational Runbook
11. Future Enhancements & Roadmap
12. Revision History

---

## 1Â Â Objectives & Success Metrics

| Objective                               | Success Metric                                               | Target  |
| --------------------------------------- | ------------------------------------------------------------ | ------- |
| Continuous improvement of agent quality | **â‰¥â€¯2â€¯pp weekly reduction** in manual override rate          | 2â€¯pp/wk |
| Latency preservation                    | p95 inference latency increase **â‰¤â€¯5â€¯%** vs previous adapter | â‰¤â€¯+5â€¯%  |
| Cost control                            | Training GPU cost **â‰¤â€¯\$150â€¯/â€¯night** (Pilot)                | <\$150  |

---

## 2Â Â Data Sources & Privacy Classification

| Source Table          | Data Class | Retention | Anonymisation / Masking                |
| --------------------- | ---------- | --------- | -------------------------------------- |
| `workflow_event`      | Internal   | 365â€¯days  | Strip `workspace_id` before training   |
| `expense` (text cols) | Restricted | 180â€¯days  | Tokenise PII fields, keep category IDs |
| `agent_trace` (JSON)  | Internal   | 90â€¯days   | Remove user identifier fields          |
| Public eval set       | Public     | âˆž         | Synthetic                              |

*Data extraction queries live in ****\`\`****.*

---

## 3Â Â Feature Engineering & Dataset Build

1. **SQL Extraction** â€“ GitHub Action `extract_dataset.yml` runs nightly against Supabase readâ€‘replica.
2. **Parquet Dump** â€“ Write to Supabase storage `s3://datasets/{{date}}/raw.parquet`.
3. **Feature Pipeline (PySpark)** â€“ Jobs on Fly.io GPU node:
   - Tokenise text fields with `tiktoken` BPE.
   - Compute `reward = 1 â€“ override_flag`.
   - Balance classes via upâ€‘sampling (max 3Ã— minority).
4. **Train/Val/Test Split:** 80â€¯/â€¯10â€¯/â€¯10 by `record_date` to avoid leakage.

---

## 4Â Â Model Architecture & Adapter Strategy

| Component           | Choice                                   | Rationale                                 |
| ------------------- | ---------------------------------------- | ----------------------------------------- |
| **Base Model**      | `Mistralâ€‘7Bâ€‘Instruct` (Apacheâ€¯2)         | Openâ€‘weights, good cost/perf, 8â€¯GB quant. |
| **Adapter Method**  | QLoRA (4â€‘bit)                            | Minimises GPU RAM; fast nightly tunes.    |
| **Prompt Format**   | Mixture of System / User / Tool messages | Aligns with LangChain chat template.      |
| **Parameter Count** | \~85â€¯M adapter params                    | Fits A10G 24â€¯GB GPU in <â€¯8â€¯GB.            |

Hyperâ€‘parameters (default):

```
learning_rate = 8eâ€‘5
batch_size     = 128
epochs         = 1
warmup_steps   = 50
lora_rank      = 64
```

---

## 5Â Â Fineâ€‘Tune Orchestration Pipeline

```
GitHub Actions (train_adapters.yml)
â””â”€â–º Fly Job (Docker) â€“ trainer.py
      â”œâ”€ sync dataset (Supabase)
      â”œâ”€ ðŸ¤— Accelerate fineâ€‘tune
      â”œâ”€ eval.py on val set
      â”œâ”€ upload adapter â†’ Supabase storage /adapters/{date}.safetensors
      â””â”€ write run row â†’ model_registry
```

*Estimated runtime*: 45â€¯min (A10G), costÂ â‰ˆâ€¯\$2.10/hr.

> **Prompt BlockÂ ML1â€‘TRAINâ€‘CMD** â€“ "Generate CLI command for QLoRA fineâ€‘tune on model {{model}}, lr {{lr}}, epochs {{ep}}."

---

## 6Â Â Evaluation & Safety Filters

1. **Automated Metrics** â€“ exactâ€‘match accuracy, ROUGEâ€‘L, mean reward.
2. **Safety Classifier** â€“ Lightweight textâ€‘classification head flags PII leakage, hateful content (thresholdÂ >â€¯0.4).
3. **Regression Benchmarks** â€“ `benchmarks/*.json` run via `lmâ€‘evalâ€‘harness`; must not lose >â€¯1â€¯pp accuracy on critical tasks.
4. **Human spotâ€‘check** â€“ 25 random samples reviewed by QA weekly.

Promotion rule (pseudoâ€‘SQL):

```sql
SELECT promote_if(
  accuracy_delta  > 0.5 AND
  override_delta  < -1.0 AND
  safety_flag_pct < 0.1 AND
  latency_delta   < 0.05
);
```

---

## 7Â Â Model Registry & Promotion Policy

| Column         | Type        | Description                     |
| -------------- | ----------- | ------------------------------- |
| `adapter_id`   | UUID        | Primary key                     |
| `created_at`   | TIMESTAMPTZ | Training completion time        |
| `metrics_json` | JSONB       | Accuracy, reward, latency, cost |
| `safety_flags` | INT         | Offending samples               |
| `status`       | TEXT        | `candidate`, `prod`, `archived` |

Promotion flow:

1. Candidate meets rule â†’ status `prod`.
2. Supabase Edge Function rotates `MODEL_ENDPOINT` version.
3. Old prod adapter archived after 7â€¯days.

Rollback: change status of previous adapter to `prod`; Edge Function picks up.

---

## 8Â Â Cost Model & GPU Resource Planning

| Tier       | GPU Type | Jobs/night | GPUâ€‘hours | \$â€¯/â€¯night | Notes             |
| ---------- | -------- | ---------- | --------- | ---------- | ----------------- |
| Pilot      | A10G     | 1          | 0.75      | \$1.60     | Onâ€‘demand spot    |
| Growth     | A10G     | 1          | 1.25      | \$2.70     | Larger dataset    |
| Enterprise | L40S     | 1          | 2.0       | \$4.80     | 2Ã— epochs, 16â€‘bit |

Budget guardâ€‘rail Prometheus alert: `gpu_train_cost_usd_total > 200` (30â€‘day rollâ€‘up).

---

## 9Â Â Prompt Blocks & Automation Scripts

| ID                | Purpose                                                       |
| ----------------- | ------------------------------------------------------------- |
| ML1â€‘SQLâ€‘GEN       | Generate SQL extraction query for dataset slice.              |
| ML1â€‘TRAINâ€‘CMD     | Build HuggingÂ Face fineâ€‘tune CLI command.                     |
| ML1â€‘EVALâ€‘PROMPT   | Draft System+User prompt for regression benchmark creation.   |
| ML1â€‘REGISTRYâ€‘TEST | Produce pgTAP tests to validate model\_registry update logic. |

---

## 10Â Â Operational Runbook

1. **Training Fail** â€“ GitHub Action fails; alert Slack `#mlâ€‘ops`; retry with `rerun-failed`.
2. **Safety Flag Surge** â€“ Adapter blocked; investigate new data anomalies; patch filters.
3. **Latency Regress >â€¯5â€¯%** â€“ Rollback to previous adapter; open Jira performance ticket.
4. **Cost Alert** â€“ Review GPU utilisation; switch to spot or scale batch size down.

---

## 11Â Â Future Enhancements & Roadmap

- Migrate to **Flashâ€‘AttentionÂ 2** kernels to halve compute time.
- Explore **Mixtureâ€‘ofâ€‘Experts (MoE)** adapters to reduce inference latency.
- Introduce **continual learning** on streaming data (River library) for online updates.
- Add **metadataâ€‘conditioned training** (tenant vertical) for vertical specialisation.

---

## 12Â Â Revision History

| Version | Date       | Author         | Notes                                       |
| ------- | ---------- | -------------- | ------------------------------------------- |
| 0.1     | 2025â€‘06â€‘12 | Perâ€¯Swedenborg | Initial comprehensive draft of ML pipeline. |

