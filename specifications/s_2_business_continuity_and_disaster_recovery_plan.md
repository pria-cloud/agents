# S2 — Business‑Continuity & Disaster‑Recovery Plan (BCP/DR)

**Document status:** Draft v0.1 · June 2025\
**Owner:** CEO (Per Swedenborg)\
**Purpose:** Define the strategies, procedures, and controls that enable PRIA to maintain or rapidly restore critical services in the event of major disruptions—natural disasters, cyber‑attacks, cloud outages, or pandemic‑level workforce interruptions.  The plan satisfies SOC 2 (CC7.3), ISO 27001 (Annex A.17), and supports customer contractual RPO/RTO commitments.

---

## Table of Contents

1. BIA Summary & Critical Functions
2. Recovery Objectives (RPO / RTO Matrix)
3. Continuity Strategies (People, Process, Technology)
4. Disaster‑Recovery Architecture
5. Backup & Replication Policies
6. DR Execution Procedures
7. Communication Plan (Internal & External)
8. Testing & Validation Schedule
9. Governance, Roles & Responsibilities
10. Prompt Blocks & Automation Scripts
11. Future Enhancements & Roadmap
12. Revision History

---

## 1  Business‑Impact Analysis (BIA) Summary

| Critical Function          | Dependency Stack                | Max Tolerable Downtime (MTD) | RTO Target | RPO Target |
| -------------------------- | ------------------------------- | ---------------------------- | ---------- | ---------- |
| **Workflow Orchestration** | Fly apps + Supabase Postgres    | 4 h                          | 30 min     | 5 min      |
| **API & Edge Web**         | Vercel Edge, Supabase PostgREST | 8 h                          | 1 h        | 15 min     |
| **AI Inference Service**   | Fly GPU pool, vLLM adapters     | 12 h                         | 2 h        | 30 min     |
| **Marketplace Billing**    | Stripe Connect, Supabase        | 24 h                         | 4 h        | 4 h        |
| **Observability Stack**    | Grafana Cloud, Loki, Tempo      | 48 h                         | 12 h       | 24 h       |

---

## 2  Recovery Objective Matrix

| Tier           | RTO    | RPO    | Contractual Notes                 |
| -------------- | ------ | ------ | --------------------------------- |
| **Pilot**      | 2 h    | 15 min | No SLA penalty, goodwill rebates  |
| **Growth**     | 1 h    | 10 min | SLA credit 5 % MRR per hour late  |
| **Enterprise** | 30 min | 5 min  | SLA credit 10 % MRR per hour late |

---

## 3  Continuity Strategies

### 3.1  People

- **Work‑from‑Anywhere** default; all critical staff have LTE fallback hotspots.
- **BCP Call Tree** auto‑generated by PagerDuty `bcp.json`.
- **Cross‑Training**: at least two engineers per critical component.

### 3.2  Process

- **Run‑book‑as‑Code** in `runbooks/` with version control.
- **Tabletop Exercises** quarterly; inject realistic disruption scenarios.
- **After‑Action Reviews** feed into S1 control improvements.

### 3.3  Technology

| Component      | Continuity Mechanism                                                                        |
| -------------- | ------------------------------------------------------------------------------------------- |
| Supabase DB    | **Point‑in‑Time Recovery (PITR)** 7‑day WAL; hourly base‑backup to EU‑North‑2.              |
| Fly Apps       | Active‑active regions `yyz` (primary) & `ams` (secondary) with TCP load‑balancer fail‑over. |
| Vercel Edge    | Multi‑PoP by default; DNS failover via NS1.                                                 |
| Object Storage | Supabase S3 multi‑AZ; nightly sync to Backblaze.                                            |
| GPU Inference  | Blue/Green pools; spot pool → on‑demand pool failover.                                      |

---

## 4  Disaster‑Recovery Architecture

```
          ┌───────────── EU Central (Primary) ───────────────┐
          │ Supabase (AZ‑A,B)  │ Fly Apps (yyz) │ GPU Pool   │
Internet ─┤                 NS1 DNS / Anycast Edge            ├─> Users
          └─────────▲────────────┬────────────▲──────────────┘
                    │ Fail‑over  │            │ DB WAL ship
          ┌─────────┴────────────▼────────────┴──────────────┐
          │          EU North 2 (Secondary)                  │
          │ Supabase read‑rep  │ Fly Apps (ams) │ GPU Pool    │
          └───────────────────────────────────────────────────┘
```

Failover triggers: Supabase health → RDS‑style replica promote API; Fly Global Accelerator weight shift; NS1 DNS failover (TTL = 30 s).

---

## 5  Backup & Replication Policies

| Asset                     | Method                    | Frequency   | Retention | Storage Class | Encryption  |
| ------------------------- | ------------------------- | ----------- | --------- | ------------- | ----------- |
| Postgres WAL + base       | Supabase PITR             | WAL 5 min   | 7 days    | S3‑Standard   | AES‑256‑KMS |
| Object storage (receipts) | Supabase → Backblaze sync | Nightly     | 30 days   | B2 Hot        | AES‑256‑SSE |
| Grafana dashboards        | API export script         | Weekly      | 13 months | S3‑Standard   | AES‑256‑KMS |
| Terraform state           | Remote backend            | Each apply  | 13 months | S3‑Versioned  | AES‑256‑KMS |
| LLM adapter weights       | Supabase storage          | After train | 90 days   | S3‑StandardIA | AES‑256‑KMS |

Integrity verified via SHA‑256 checksum; backup logs stored in Loki label `backup=true`.

---

## 6  DR Execution Procedures

### 6.1  Supabase Primary Failure

1. **Detect** – Grafana alert `pg_down > 0`.
2. **Promote Replica** – API `supabase.db.promote_replica(region='eu-north-2')`.
3. **Update Secrets** – Edge Config key `DB_URL` → replica endpoint.
4. **Run Smoke Tests** – `make smoke db`.
5. **Communicate** – Statuspage component `Database`.

### 6.2  Fly Region Outage

1. **Scale Secondary** – `fly scale set ams 4`.
2. **Shift Traffic** – GA weight `yyz=0`, `ams=100`.
3. **Disable Deploys** – GitHub environment `growth` locked.
4. **Evaluate Capacity** – If CPU > 70 %, add `ams` machines.

### 6.3  Full Cloud Provider Outage (Black Swan)

1. Invoke “**Chaos‑Phoenix**” script: Terraform apply alt‑cloud blueprint (Hetzner + PlanetScale).
2. DNS cut‑over via NS1 traffic steering.
3. Activate reduced‑function mode (read‑only workflows) until RTO achieved.

---

## 7  Communication Plan

| Audience          | Channel              | Cadence                   | Owner        |
| ----------------- | -------------------- | ------------------------- | ------------ |
| Internal team     | Slack `#inc‑channel` | Live, every 15 min        | Incident Cmd |
| Customers         | Statuspage, email    | First 30 min, then 60 min | PM on‑call   |
| Regulators (GDPR) | Email DPO authority  | Within 72 h for breach    | DPO          |
| Investors         | CEO email            | Post‑incident summary     | CEO          |

Templates stored in `/comm-templates/*.md`.

---

## 8  Testing & Validation Schedule

| Test Type           | Frequency   | Owner          | Evidence Location                |
| ------------------- | ----------- | -------------- | -------------------------------- |
| Tabletop Exercise   | Quarterly   | Security Lead  | Confluence `BCP Exercises` space |
| Failover Drill      | Semi‑annual | DevOps Lead    | Grafana runbook log + Loki trace |
| Backup Restore Test | Monthly     | DBA            | `restore_test.log` in Loki       |
| Chaos Engineering   | Annual      | Eng Leadership | Gremlin report in Confluence     |

---

## 9  Governance, Roles & Responsibilities

| Role                   | Responsibility                                 |
| ---------------------- | ---------------------------------------------- |
| **BCP Owner (CISO)**   | Maintain plan, schedule tests, audit evidence. |
| **Incident Commander** | Lead during DR execution.                      |
| **Comms Lead (PM)**    | Statuspage, customer comms.                    |
| **Recovery Lead**      | Executes technical failover steps.             |
| **Audit Coordinator**  | Uploads evidence to Drata.                     |

RACI aligns with OPS1 Incident Response Playbook; BCP Owner reviews metrics quarterly.

---

## 10  Prompt Blocks & Automation Scripts

| ID                  | Purpose                                               |
| ------------------- | ----------------------------------------------------- |
| S2-DR-CHECKLIST-GEN | Generate Markdown checklist for failover scenario.    |
| S2-BACKUP-VERIFY    | Bash script; restores latest backup to temp DB; diff. |
| S2-REGULATOR-NOTIFY | Draft GDPR breach notice from incident JSON.          |

Environment variables: `DR_BACKUP_BUCKET`, `SUPABASE_PROMOTE_API_KEY`, `NS1_API_TOKEN`, `CHAOS_PHOENIX_PLAN`.

---

## 11  Future Enhancements & Roadmap

- Fully automate **Cross‑Cloud DR** leveraging Litestream for Postgres streaming to S3 (Q3 2026).
- Adopt **Cross‑Region GPU replication** using RunPod in 2026.
- Integrate **Back‑up immutability** (object‑lock) for 7‑year archive compliance.

---

## 12  Revision History

| Version | Date       | Author         | Notes                                       |
| ------- | ---------- | -------------- | ------------------------------------------- |
| 0.1     | 2025‑06‑12 | Per Swedenborg | Initial comprehensive draft of BCP/DR Plan. |

